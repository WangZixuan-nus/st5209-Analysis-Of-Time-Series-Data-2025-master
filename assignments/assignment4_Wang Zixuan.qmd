---
title: "ST5209/X Assignment 4"
format: pdf
editor: visual
author: "Wang Zixuan,A0298286H"
date: "2025-03"
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
fontsize: 12pt
mainfont: "Times New Roman"
header-includes:
  - \usepackage{titlesec}
  - \usepackage{color}
  - \usepackage{xcolor}
  - \definecolor{DarkRed}{rgb}{0.6,0,0}
  - \definecolor{DarkBlue}{rgb}{0,0,0.6}
  - \titleformat{\section}{\Large\bfseries\color{DarkBlue}}{\thesection}{1em}{}
  - \titleformat{\subsection}{\large\bfseries\color{DarkBlue}}{\thesubsection}{1em}{}
  - \titleformat{\subsubsection}{\normalsize\bfseries\color{DarkBlue}}{\thesubsubsection}{1em}{}
  - \setlength{\parindent}{0em}
  - \setlength{\parskip}{1em}
geometry: margin=1in
---

## Set up

1.  Make sure you have the following installed on your system: $\text{\LaTeX}$, R4.2.2+, RStudio 2023.12+, and Quarto 1.3.450+.
2.  Pull changes from the course [repo](https://github.com/yanshuotan/st5209-2024).

## Submission

1.  Render the document to get a .pdf printout.
2.  Submit both the .qmd and .pdf files to Canvas.

```{r}
#| message: false
#| warning: false
library(tidyverse)
library(fpp3)
library(fable)
library(tsibble)
```

## 1. AR polynomial

Consider the AR(2) model $$
X_t = 4 + 0.5 X_{t-1} - 0.25 X_{t-2} + W_t.
$$ {#eq-ar2}

a.  What is the autoregressive polynomial?

    The model can be written to:

    $$ X_t-0.5X_{t-1}+0.25X_{t-2}=4+W_t $$The autoregressive polynomial:\
    $$ \phi(z)=1-0.5z+0.25z^2. $$

b.  What are its roots?

    $\phi(z)=0$, then $1-0.5z+0.25z^2=0$.

    $$\text{ we can get the roots: }
    z_1=1+\sqrt{3}i,z_2=1-\sqrt{3}i
    $$

c.  Is this model causal? Why?

    $$
    |z_1|=|z_2|=2
    $$

    The roots both lie outside the unit disc, we can conclude that the model is causal.

d.  Given the representation $$
    X_t = \mu + \sum_{j=0}^\infty \psi_j W_{t-j},
    $$ solve for $\psi_0, \psi_1, \psi_2, \psi_3$. *Hint: Follow the strategy discussed in `linear_process_coefs.qmd`.*

    $$
    X_t = 4 + 0.5X_{t-1} - 0.25X_{t-2} + W_t.
    $$

    The AR polynomial is

    $$
    \phi(z)=1-0.5z+0.25z^2.
    $$

    The infinite MA representation is given by

    $$
    \psi(z)=\sum_{j=0}^\infty \psi_j z^j,
    $$

    these coefficients satisfy

    $$
    \phi(z)\psi(z)=1.
    $$

    $$
    \left(1-0.5z+0.25z^2\right)\left(\psi_0+\psi_1z+\psi_2z^2+\psi_3z^3+\cdots\right)=1.
    $$

    First, the constant term = 1.\
    $$
    \psi_0 = 1.
    $$

    Secondly, for the first polynomial coefficient, $$
    -0.5\psi_0 + \psi_1 = 0.
    $$ $$
    \text{then }\psi_1 = 0.5.
    $$

    Then, $$
    0.25\psi_0 - 0.5\psi_1 + \psi_2 = 0.
    $$\
    $$
    \text{since }\psi_0=1,\psi_1 = 0.5\\0.25(1)-0.5(0.5)+\psi_2 = 0\\ \text{then } 0.25-0.25+\psi_2 = 0\\ \psi_2=0
    $$ Similarly, $$
    0.25\psi_1 - 0.5\psi_2 + \psi_3 = 0.
    $$ $$
    \text{since }\psi_1 = 0.5,\psi_2=0\\
    0.25(0.5) + \psi_3 = 0 \\ 0.125 + \psi_3 = 0\\
    \psi_3=-0.125
    $$ Thus, the first four coefficients are:

    $$
    {\psi_0=1,\\ \psi_1=0.5,\\ \psi_2=0,\\ \psi_3=-0.125.}
    $$

## 2. Likelihood

Consider the AR(1) model $$
X_t = 1-0.6X_{t-1} + W_t,
$$ where $W_t \sim WN(0, 0.25)$ is Gaussian white noise. We are given observations $x_1 = 0.2, x_2 = -0.3, x_3 = 0.4$.

a.  What is the mean of $X_t$?

    Take average of both two sides of the equations:

    $$
    E(X_t)=1-0.6E(X_{t-1})+0
    $$

    Since

    $$
    E(X_t)=E(X_{t-1})=\mu\\
    1.6\mu = 1\\
    \mu=\frac{1}{1.6} =0.625
    $$

b.  Write the conditional likelihood of this model, when conditioning on the value of $x_1$.

Since $W_t\sim WN(0,0.25) ,X_t|X_{t-1}=x_{t-1}\sim N(1-0.6x_{t-1},0.25)$

$$
L_c(\beta)=\prod_{t=2}^{3}f_{\beta}(x_t|x_{t-1})
$$

where $f_{\beta}(x_t|x_{t-1})=\frac{1}{\sqrt{2\pi \sigma^2}}\text{exp}(-\frac{(x_t-(1-0.6x_{t-1}))^2}{2\sigma^2})$

with $\sigma^2=0.25$.

```{r}
x1 <- 0.2
x2 <- -0.3
x3 <- 0.4
sigma2 <- 0.25
phi <- -0.6
mu <- 0.625

fx3_12 <- (2*pi*sigma2)^-(1/2)*exp(-(x3-phi*(x2-mu)-mu)^2/(2*sigma2))
fx2_1 <- (2*pi*sigma2)^-(1/2)*exp(-(x2-phi*(x1-mu)-mu)^2/(2*sigma2))
fx1 <- (2*pi*sigma2/(1-phi^2))^-(1/2)*exp(-(x1-mu)^2/(2*sigma2/(1-phi^2)))

fx3_12*fx2_1*fx1
```

## 3. Reversibility

a.  Using `set.seed(5209)` to fix a random seed, create a sample trajectory of length $n = 500$ from the AR(2) model from Problem 1 using `arima.sim()`.

    ```{r}
    set.seed(5209)
    mu <- 4/(1-0.5+0.25)
    wn <- rnorm(200)
    ar2 <- arima.sim(model = list(ar = c(0.5, -0.25)), n = 200, innov = wn) + mu
    ```

b.  Reverse the time index of the vector you obtain using `rev()`.

    ```{r}
    rev_sample <- rev(ar2)
    ```

c.  Fit an AR(2) model to the reversed time series using `fable`. Hint: You may use the code snippet `model(AR(X ~ order(2)))`.

    ```{r}
    ts_rev <- tsibble(time = 1:200, X = rev_sample, index =time)
    fit_rev <- model(ts_rev, AR(X ~ order(2)))
    ```

d.  Inspect the model parameters using `tidy()`. Why are they similar to the those in @eq-ar2?

    ```{r}
    fit_rev |>
      tidy()
    ```

    The parameters are similar to those in eq-ar2. Since AR models with real coefficients are time-reversible. The autoregressive polynomial $1-0.5z+0.25z^2$ remains the same under time reversal, implying the reversed process has the same structure.

e.  Make a forecast with $h = 10$. What does this correspond to in terms of the original time series?

    ```{r}
    rev_fc <- fit_rev |>
      forecast(h = 10)
    rev_fc
    rev_fc |>
      autoplot(ts_rev)
    ```

Since the model is for the reversed time series, forecasting for 10 horizons corresponds to predicting: $X_0,X_{-1},\cdots,X_{-9}$.

## 4. Yule-Walker

a.  Write the Yule-Walker equations for the AR(2) model from Problem 1.

    AR(2) model:

    $$
    X_t = 4 + 0.5 X_{t-1} - 0.25 X_{t-2} + W_t.
    $$

    Recall that for an AR(2) model written in centered form

    $$
    X_t - \mu = \phi_1 (X_{t-1}-\mu) + \phi_2 (X_{t-2}-\mu) + W_t,
    $$

    the Yule–Walker equations for the autocovariances are given by:

    1.  For lag ( h = 0 ): $$
        \gamma(0) = \phi_1 \gamma(1) + \phi_2 \gamma(2) + \sigma_W^2,
        $$

    2.  For lag ( h = 1 ): $$
        \gamma(1) = \phi_1 \gamma(0) + \phi_2 \gamma(1).
        $$

    3.  For lag ( h \>= 2 ) , the relationship is: $$
        \gamma(h) = \phi_1 \gamma(h-1) + \phi_2 \gamma(h-2).
        $$

    For the specific AR(2) model given by

    $$
    X_t = 4 + 0.5 X_{t-1} - 0.25 X_{t-2} + W_t,
    $$

    the parameters are ( $\phi_1 = 0.5 \text{ and } \phi_2 = -0.25 )$. Ignoring the constant, the Yule–Walker equations become:

    -   For ( h = 0 ): $$
        \gamma(0) = 0.5\, \gamma(1) - 0.25\, \gamma(2) + \sigma_W^2.
        $$

    -   For ( h = 1 ): $$
        \gamma(1) = 0.5\, \gamma(0) - 0.25\, \gamma(1).
        $$

    -   And for ( h = 2 ): $$
        \gamma(2) = 0.5\, \gamma(1) - 0.25\, \gamma(0).
        $$

b.  Arrange the equations in the following matrix form (i.e. fill in the missing entries): $$
    \left[\begin{matrix}     ? & ? & ?  \\ ? & ? & ? \\
    ? & ? & ?
    \end{matrix}\right] \left[\begin{matrix}     \gamma(0) \\ \gamma(1) \\ \gamma(2)
    \end{matrix}\right] = \left[\begin{matrix}     \sigma^2 \\ 0 \\ 0
    \end{matrix}\right].
    $$

$$
\left[\begin{matrix} 
    1 & -0.5 & 0.25  \\ 
    -0.5 & 1.25 & 0 \\ 
    0.25 & -0.5 & 1 
\end{matrix}\right] 
\left[\begin{matrix} 
    \gamma(0) \\ 
    \gamma(1) \\ 
    \gamma(2)
\end{matrix}\right] = 
\left[\begin{matrix} 
    \sigma^2 \\ 
    0 \\ 
    0 
\end{matrix}\right].
$$

a.  Solve the system from part b) for $\gamma(0), \gamma(1), \gamma(2)$ numerically using `solve()`.

    ```{r}
    A <- matrix(c(1, -0.5, 0.25,
                  -0.5, 1.25, 0,
                  0.25, -0.5, 1),nrow = 3, byrow = TRUE)
    # let sigma^2 = 1
    sigma2 <- 1
    b <- c(sigma2, 0, 0)
    gamma <- solve(A, b)

    cat(sprintf("value of gamma(0) is %.4f\n", gamma[1]))
    cat(sprintf("value of gamma(1) is %.4f\n", gamma[2]))
    cat(sprintf("value of gamma(2) is %.4f\n", gamma[3]))

    ```

b.  Given $$
    \Gamma_2 = \left[\begin{matrix}     \gamma(0) & \gamma(1) \\ \gamma(1) & \gamma(0)
    \end{matrix}\right],
    $$ what is the top left entry of $\Gamma_2^{-1}$ in terms of $\gamma(0)$ and $\gamma(1)$?

    $$
    \Gamma_2^{-1}=\frac{1}{|\Gamma_2|}
    \left[\begin{matrix}     
    \gamma(0) & -\gamma(1)  \\ 
    -\gamma(1) & \gamma(0)
    \end{matrix}\right]
    $$

    $$
    |\Gamma_2|=\gamma(0)^2-\gamma(1)^2
    $$

    So the top left entry $\Gamma_2^{-1}$is $\frac{\gamma(0)}{\gamma(0)^2-\gamma(1)^2}$.

c.  Write a 95% confidence interval for $\phi_1$ using your answers for d) and 3d).

    ```{r}
    #tidy(fit_rev)$estimate
    phi1 <- tidy(fit_rev)$estimate[2]
    sigma2 <- 1
    gamma <- solve(A, b)

    # Compute the variance
    variance_1 <- sigma2/200 * (gamma[1] / (gamma[1]^2 - gamma[2]^2))

    # standard error for phi_1
    sd_phi1 <- sqrt(variance_1)

    c1 <- round(phi1 - 1.96 * sd_phi1,4)
    c2 <- round(phi1 + 1.96 * sd_phi1,4)


    cat("the confidence intervel is : [", c1, ",", c2, "]\n")
    ```

## 5. Recursive forecasting for ARMA(1, 1)

Consider the $ARMA(1,1)$ model $$
X_t - 0.5X_{t-1} = W_t + 0.5W_{t-1}.
$$ In this question, we will investigate recursive forecasting. The following code snippet generates a sequence of length $n=50$ drawn from the above model.

```{r}
set.seed(5209)
n <- 50
wn <- rnorm(n)
xt <- arima.sim(model = list(ar = 0.5, ma = 0.5), innov = wn, n = n)
```

a.  Fill in the following code snippet using equation (12.14) to generate a sequence `wn_hat`.

```{r}
wn_hat <- rep(0, n)
wn_hat[[1]] <- xt[[1]]
for (i in 2:n) {
  # FILL IN
  wn_hat[i] <- xt[i] - 0.5 * xt[i-1] - 0.5 * wn_hat[i-1]
}
```

b.  Make a time plot of the log absolute difference between `wn` and `wn_hat`.

    ```{r}
    log_diff <- log(abs(wn - wn_hat))

    plot(log_diff, type = "l", col = "blue", lwd = 2,
         xlab = "Time", ylab = "log(|wn - wn_hat|)",
         main = "Time Plot of log(|wn - wn_hat|)")
    ```

c.  What consequence does this have for truncated forecasts?

    Truncated forecasts can underestimate the forecast error variance, leading to prediction intervals that are too narrow because they ignore the effects of long-run (but diminishing) influences from past shocks.

d.  Compute the truncated forecast for $X_{53}$.

    ```{r}
    x51 <- 0.5 * xt[50] + 0.5 * wn_hat[50]
    x52 <- 0.5 * x51
    x53 <- 0.5 * x52; x53
    ```

## 6. ACF, PACF, and BLPs

Let $(X_t)$ be a mean zero stationary process with the following autocovariance values:

$$
\gamma_X(0) = 2, \gamma_X(1) = 1.4, \gamma_X(2) = 0.6, \gamma_X(3) = 0.4, \gamma_X(4) = 0.2.
$$

a.  Can $(X_t)$ be an MA(2) process? Explain why or why not.

    The presence of nonzero autocovariances for lags 3 and 4 means that the process cannot have a finite moving average representation of order 2.

b.  Can $(X_t)$ be an AR(1) process? Explain why or why not.

    No, (X_t) cannot be an AR(1) process. Since these ratios are not equal, the decay is not geometric and thus (X_t) cannot be modeled as an AR(1) process.

    For an AR(1) process, the autocovariance function decays geometrically\
    $$\gamma_X(h) = \phi^h \, \gamma_X(0),$$which implies that the ratio of successive autocovariances is constant\
    $$\frac{\gamma_X(1)}{\gamma_X(0)} = \phi \quad \text{and} \quad \frac{\gamma_X(2)}{\gamma_X(1)} = \phi.$$Given DataThe autocovariance values for X_t are provided as\
    $$\gamma_X(0) = 2, \quad \gamma_X(1) = 1.4, \quad \gamma_X(2) = 0.6.$$<!-- -->1. For lag 1: $$   \phi_1 = \frac{\gamma_X(1)}{\gamma_X(0)} = \frac{1.4}{2} = 0.7.   $$ 2. For lag 2: $$   \phi_2 = \frac{\gamma_X(2)}{\gamma_X(1)} = \frac{0.6}{1.4} \approx 0.4286.   $$

    Therefore, the process cannot be modeled as an AR(1) process. No, X_t cannot be an AR(1) process. Since these ratios are not equal, the decay is not geometric and thus X_t cannot be modeled as an AR(1) process.

c.  What is the best linear predictor $\hat X_4$ for $X_4$ given only $X_3 = 2$?

    $$ \hat X_4=\frac{\gamma_X(1)}{\gamma_X(0)}X_3=0.7 *2=1.4 $$

d.  Using the notation in part c), what is the variance of $X_4 - \hat X_4$?

    The variance of the prediction error, $Var(X_4− \hat{H}X_4) = \gamma(0) − \gamma_4^TΓ^{-1}\gamma_4$

    $$ \text{Var}(X_4-\hat X_4)=\text{Var}(X_4-\frac{\gamma(1)}{\gamma(0)}）=\gamma(0)(1-(\frac{\gamma(1)^2}{\gamma(0)}))=2*(1-0.7^2)=1.02 $$

e.  What is the best linear predictor $\hat X_4$ for $X_4$ given only $X_2 = 2$?

    Since the process is mean zero and stationary

    $$
    \hat X_4=\frac{\gamma(2)}{\gamma(0)}X_2=\frac{0.6}{2}*2=0.6
    $$

f.  Using the notation in part e), what is the variance of $X_4 - \hat X_4$?

    $$
    \text{Var}(X_4-\hat X_4)=\gamma_X(0)(1-\rho_X^2(2))=\gamma(0)(1-(\frac{\gamma(2)}{\gamma(0)})^2)=2*(1-0.3^2)=1.82
    $$

g.  Let $\alpha_X$ denote the partial autocorrelation function of $(X_t)$. What is $\alpha_X(1)$?

    The partial autocorrelation at lag 1 is defined as

    $$
    \alpha_X(1)=\gamma_X(1)/\gamma_X(0)\\
    =1.4/2=0.7
    $$

h.  What is $\alpha_X(3)$?

    The partial autocorrelation at lag 3, denoted by $\alpha_X(3)$, can be calculated using the innovations algorithm. In particular, if we let $\phi_{3,3}$ be the corresponding coefficient when predicting $X_3$ from its past values, then

      $\alpha_X(3) = \phi_{3,3}$

```{r}
A <- matrix(c(2, 1.4, 0.6,
              1.4, 2, 1.4,
              0.6, 1.4, 2), nrow = 3, byrow = TRUE)
b <- c(1.4, 0.6, 0.4)

phi <- solve(A, b)
phi3_3 <- phi[3]
alpha_x3 <- phi3_3  

cat("The partial autocorrelation alpha_X(3) is approximately:", 
    round(alpha_x3, 4), "\n")
```

---
title: "Week 7 Demonstration"
format: html
editor: visual
---

## Set up

```{r}
#| message: FALSE
library(fpp3)
library(tidyverse)
library(slider)
library(gridExtra)
```

## 1. ACVF, ACF, and stationarity

Consider the time series defined as $$
X_t = U_1 \sin(2\pi \omega t) + U_2\cos(2\pi\omega t),
$$ where $U_1$ and $U_2$ are independent random variables with zero mean and variance 1.

a.  Compute the ACVF of $(X_t)$.
b.  Is $(X_t)$ weakly stationary?
c.  Is $(X_t)$ strictly stationary? If not, what conditions can we put on $U_1$ and $U_2$ to make it stationary?

$$
\begin{split}
\gamma_X(s, t) & = \text{Cov}\lbrace X_t,X_s\rbrace \\
& = \text{Cov}\lbrace U_1 \sin(2\pi \omega s) + U_2\cos(2\pi\omega s), U_1 \sin(2\pi \omega t) + U_2\cos(2\pi\omega t)\rbrace \\
& = \sin(2\pi \omega s)\sin(2\pi\omega t)\text{Cov}\lbrace U_1, U_1 \rbrace + \sin(2\pi\omega s)\cos(2\pi\omega t)\text{Cov}\lbrace U_1, U_2 \rbrace \\
& \quad\quad + \cos(2\pi\omega s)\sin(2\pi\omega t)\text{Cov}\lbrace U_2, U_1 \rbrace + \cos(2\pi\omega s)\cos(2\pi\omega t)\text{Cov}\lbrace U_2, U_2 \rbrace \\
& = \sin(2\pi\omega s)\sin(2\pi\omega t) + \cos(2\pi\omega s)\cos(2\pi\omega t) \\
& = \cos(2\pi\omega(t-s))
\end{split}
$$

Here, the last equality uses a triogonometric identity. Alternatively, can think of this as the dot product between two unit vectors.

Observe that $\gamma_X(s, t)$ depends only on the difference $t-s$. We further have $$
\mu_X(t) = \mathbb{E}\lbrace U_1 \sin(2\pi \omega t) + U_2\cos(2\pi\omega t) \rbrace = 0,
$$ which does not depend on $t$. As such, $(X_t)$ is weakly stationary.

On the other hand, depending on the choice of $U_1, U_2$, it may not be strongly stationary. For instance, let $U_1$ and $U_2$ have the distribution given by $$
\mathbb{P}\lbrace U = 1\rbrace = \mathbb{P}\lbrace U = -1 \rbrace = 1/2.
$$ This is known as the Rademacher distribution. Suppose also that $\omega = 1/8$. We then have $$
X_2 = U_1\sin(\pi/4) + U_2\cos(\pi/4) = U_1. 
$$ On the other hand, $$
X_1 = U_1\sin(\pi/4) + U_2\cos(\pi/4) = U_1/\sqrt{2} + U_2/\sqrt{2}. 
$$ As such, we have $$
\mathbb{P}\lbrace X_1 = \sqrt{2}\rbrace = \mathbb{P}\lbrace X_1 = -\sqrt{2}\rbrace = 1/4
$$ $$
\mathbb{P}\lbrace X_1 = 0\rbrace = 1/2
$$

If $U_1, U_2 \sim N(0,1)$, then the process is strictly stationary, since it is a Gaussian process, in which case, all finite dimensional distributions depend only on the mean and AVCF values.

------------------------------------------------------------------------

## 2. ACF vs sample ACF

Consider the linear trend model $$
X_t = \beta_0 + \beta_1 t + W_t.
$$

a.  Compute the ACF of $(X_t)$.
b.  Simulate a time series drawn from this model and plot its sample ACF.
c.  Why does the sample ACF not look like the population ACF function?
d.  Why does the asymptotic normality theorem for the ACF not apply?

------------------------------------------------------------------------

The ACF is $$
\gamma_X(s, t) = \text{Cov}\lbrace X_t,X_s\rbrace = \text{Cov}\lbrace W_t,W_s\rbrace = \begin{cases}
1 & \text{if}~s = t \\
0 & \text{otherwise}.
\end{cases}
$$

This hence depends only on the difference $s-t$.

```{r}
#| echo: TRUE
beta0 <- 3
beta1 <- 0.4
ts_data <-
  tibble(
    idx = 1:200,
    wn = rnorm(200),
    xt = beta0 + beta1 * idx + wn
  ) |>
  as_tsibble(index = idx)
ts_data |>
  ACF(xt) |>
  autoplot()
```

The sample ACF is calculated as $$
\hat\gamma_X(h) = \frac{1}{n}\sum_{i=1}^{n-h}(X_i - \bar X) (X_{i+h} - \bar X).
$$ The expectation of each term is roughly $$
\mathbb{E}\lbrace(X_i - (\beta_0 + \beta n/2))X_{i+h} - (\beta_0 + \beta n/2))\rbrace.
$$ This is not the covariance between them because $\beta_0 + \beta n /2$ is not their means unless $i = n/2$ or $i+h = n/2$.

The asymptotic normality theorem does not apply because it requires the time series to be stationary. Since the mean function $\beta_0 + \beta_1 t$ depends on $t$, $(X_t)$ is not stationary.

------------------------------------------------------------------------

## 3. Prediction interval widths

Consider the linear trend method, the drift method, and the seasonal naive method.

a.  What is the formula for the width of their prediction intervals as a function of the forecast horizon $h$?

b.  Use the first methods to forecast China's GDP in `global_economy` and verify this is the case. For seasonal naive, forecast arrivals from Japan in `aus_arrivals`.

------------------------------------------------------------------------

For linear trend, the distributional forecast is $\hat x_{n+h|n} \sim N( \hat \beta_0 + \hat \beta_1 (n+h), \sigma^2)$. Hence the width of a level 0.95 prediction interval is $1.96\sigma$.

For the drift method, the distributional forecast is $\hat x_{n+h|n} \sim N( x_n, h\sigma^2)$. Hence the width of a level 0.95 prediction interval is $1.96\sigma\cdot h^{1/2}$.

The seasonal naive method distributional forecast is $\hat x_{n+h|n} \sim N( x_{n-(-h)~\text{mod}~p}, \lceil h/p \rceil\sigma^2)$. Hence, the width of a level 0.95 prediction interval is $1.96\sigma\cdot \lceil h/p \rceil^{1/2}$.

```{r}
china_gdp <- global_economy |>
  filter(Country == "China") |>
  model(linear = TSLM(GDP ~ trend()),
        drift = NAIVE(GDP ~ drift())) |>
  forecast(h = 50)

plt1 <-
  china_gdp |>
  filter(.model == "linear") |>
  autoplot(global_economy)

plt2 <-
  china_gdp |>
  filter(.model == "drift") |>
  autoplot(global_economy)

plt3 <-
  aus_arrivals |>
  filter(Origin == "Japan") |>
  model(snaive = SNAIVE(Arrivals)) |>
  forecast(h = 50) |>
  autoplot(aus_arrivals)

grid.arrange(plt1, plt2, plt3)
```

------------------------------------------------------------------------

## 4. Gaussian conditional distribution in 2D

Let $(X_1,X_2)$ be a 2D Gaussian random vector with mean $\mu = (1, 1)$ and covariance matrix

$$
\Sigma = \begin{bmatrix} 2  & 2\rho \\ 2\rho & 2\end{bmatrix}
$$

a.  From first principles, derive the conditional distribution of $X_2$ given $X_1=x_1$.

b.  How does the distribution change as $\rho$ changes? How about as $x_1$ changes?

c.  Make a scatter plot of the joint distribution of $(X_1,X_2)$ for $\rho = 0.1$ and for $\rho = 0.9$ . Does this agree with your answer from (b) and (c)?

![](../_images/gaussian_conditional_distribution.jpeg)

```{r}
X <- tibble(x1 = rnorm(200),
            wn = rnorm(200),
            x2 = 0.9 * x1 + sqrt(1 - 0.9 ** 2) * wn,
            x3 = 0.1 * x1 + sqrt(1 - 0.1 ** 2) * wn)

plt1 <- X |> 
  ggplot(aes(x = x1, y = x2)) + geom_point()

plt2 <- X |> 
  ggplot(aes(x = x1, y = x3)) + geom_point()

grid.arrange(plt1, plt2)
```
